{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1614,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from pprint import pprint\n",
    "from decisiontree import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1615,
   "metadata": {},
   "outputs": [],
   "source": [
    "house = importfile('hw3_house_votes_84.csv', ',')\n",
    "housecategory = {}\n",
    "for i in house[0]:\n",
    "    housecategory[i] = 'categorical'\n",
    "housecategory[\"class\"] = 'class'\n",
    "housedata = np.array(house[1:]).astype(float)\n",
    "\n",
    "# print(len(housedata))\n",
    "# print(housecategory)\n",
    "\n",
    "#class in last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1616,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = importfile('hw3_wine.csv', '\\t')\n",
    "winecategory = {}\n",
    "for i in wine[0]:\n",
    "    winecategory[i] = 'numerical'\n",
    "winecategory[\"# class\"] = 'class'\n",
    "winedata = np.array(wine[1:]).astype(float)\n",
    "# winedata = np.array([[float(item) for item in onelist] for onelist in wine[1:]])\n",
    "\n",
    "# print(len(winedata))\n",
    "# print(winecategory)\n",
    "\n",
    "#class in first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1617,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = importfile('hw3_cancer.csv', '\\t')\n",
    "cancercategory = {}\n",
    "for i in cancer[0]:\n",
    "    cancercategory[i] = 'numerical'\n",
    "cancercategory[\"Class\"] = 'class'\n",
    "cancerdata = np.array(cancer[1:]).astype(float)\n",
    "\n",
    "#print(len(cancerdata))\n",
    "#print(cancercategory)\n",
    "\n",
    "#class in last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1618,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmc = importfile('cmc.data', ',')\n",
    "cmccategory = {\"Wife's age\":\"numerical\",\"Wife's education\":\"categorical\",\n",
    "\"Husband's education\":\"categorical\",\"Number of children ever born\":\"numerical\",\n",
    "\"Wife's religion\":\"binary\",\"Wife's now working?\":\"binary\",\n",
    "\"Husband's occupation\":\"categorical\",\"Standard-of-living index\":\"categorical\",\n",
    "\"Media exposure\":\"binary\",\"Contraceptive method used\":\"class\"}\n",
    "cmcdata = np.array(cmc).astype(int)\n",
    "\n",
    "# print(len(cmcdata))\n",
    "# print(cmccategory)\n",
    "\n",
    "#class in last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartbestseperate(dataset, attributes:dict):\n",
    "    # dataset in is the dataset by row. \n",
    "    # attributes is the dictionary of attributes:type \n",
    "    # types: numerical, categorical, binary.\n",
    "    datasetbycolumn = dataset.T\n",
    "    classindex = list(attributes.values()).index(\"class\")\n",
    "    originalgini = gini(datasetbycolumn[classindex])\n",
    "    smallestgini = originalgini\n",
    "    thresholdvalue = -1\n",
    "\n",
    "    i = 0\n",
    "    bestattribute = {list(attributes.keys())[i]:attributes[list(attributes.keys())[i]]}\n",
    "    attributesinuse = list(attributes.keys())[1:] if (classindex == 0) else list(attributes.keys())[:classindex]\n",
    "    # datasetinuse = datasetbycolumn[1:] if (classindex == 0) else datasetbycolumn[:classindex]\n",
    "\n",
    "    for attribute in attributesinuse:\n",
    "        idx = i+1 if classindex == 0 else i\n",
    "\n",
    "        if attributes[attribute] == \"categorical\" or attributes[attribute] == \"binary\":\n",
    "            listofkeys = list(Counter(datasetbycolumn[idx]).keys())\n",
    "            listofcategory = [] # this is the list of categorical values.\n",
    "            \n",
    "            for key in listofkeys:\n",
    "                indexlist = [idex for idex, element in enumerate(datasetbycolumn[idx]) if element == key]\n",
    "                category = np.array(datasetbycolumn[classindex][indexlist])\n",
    "                listofcategory.append(category)\n",
    "\n",
    "            currentgini = 0\n",
    "\n",
    "            for ctgry in listofcategory:\n",
    "                a = len(ctgry)/len(datasetbycolumn[idx]) # This is probability\n",
    "                currentgini += a * gini(ctgry)\n",
    "\n",
    "            if currentgini < smallestgini:\n",
    "                smallestgini = currentgini\n",
    "                bestattribute = {attribute:attributes[attribute]}\n",
    "            \n",
    "        elif attributes[attribute] == \"numerical\":\n",
    "            datasetsort = datasetbycolumn.T[datasetbycolumn.T[:,idx].argsort(kind='quicksort')].T\n",
    "            currentthreshold = (datasetsort[idx][1]+datasetsort[idx][0])/2\n",
    "            k = 1\n",
    "            while k < len(datasetsort.T):\n",
    "                currentthreshold = (datasetsort[idx][k]+datasetsort[idx][k-1])/2\n",
    "                listofcategory = [datasetsort[classindex][:k],datasetsort[classindex][k:]]\n",
    "                currentgini = 0\n",
    "\n",
    "                for ctgry in listofcategory:\n",
    "                    a = len(ctgry)/len(datasetbycolumn[idx]) # This is probability\n",
    "                    currentgini += a * gini(ctgry)\n",
    "\n",
    "                if currentgini < smallestgini:\n",
    "                    smallestgini = currentgini\n",
    "                    thresholdvalue = currentthreshold\n",
    "                    bestattribute = {attribute:attributes[attribute]}    \n",
    "                k += 1\n",
    "        i += 1\n",
    "\n",
    "    # set first attribution dictionary {key:type} to the best attributes.\n",
    "    return bestattribute, thresholdvalue, smallestgini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3bestseperate(dataset, attributes:dict):\n",
    "    # dataset in is the dataset by row. \n",
    "    # attributes is the dictionary of attributes:type \n",
    "    # types: numerical, categorical, binary.\n",
    "    datasetbycolumn = dataset.T\n",
    "    classindex = list(attributes.values()).index(\"class\")\n",
    "    originalentrophy = entropy(datasetbycolumn[classindex])\n",
    "    smallestentrophy = originalentrophy\n",
    "    thresholdvalue = -1\n",
    "\n",
    "    i = 0\n",
    "    bestattribute = {list(attributes.keys())[i]:attributes[list(attributes.keys())[i]]}\n",
    "    attributesinuse = list(attributes.keys())[1:] if (classindex == 0) else list(attributes.keys())[:classindex]\n",
    "    # datasetinuse = datasetbycolumn[1:] if (classindex == 0) else datasetbycolumn[:classindex]\n",
    "\n",
    "    for attribute in attributesinuse:\n",
    "        idx = i+1 if classindex == 0 else i\n",
    "\n",
    "        if attributes[attribute] == \"categorical\" or attributes[attribute] == \"binary\":\n",
    "            listofkeys = list(Counter(datasetbycolumn[idx]).keys())\n",
    "            listofcategory = [] # this is the list of categorical values.\n",
    "            \n",
    "            for key in listofkeys:\n",
    "                indexlist = [idex for idex, element in enumerate(datasetbycolumn[idx]) if element == key]\n",
    "                category = np.array(datasetbycolumn[classindex][indexlist])\n",
    "                listofcategory.append(category)\n",
    "\n",
    "            entropynow = 0\n",
    "\n",
    "            for ctgry in listofcategory:\n",
    "                a = len(ctgry)/len(datasetbycolumn[idx]) # This is probability\n",
    "                entropynow += a * entropy(ctgry)\n",
    "\n",
    "            if entropynow < smallestentrophy:\n",
    "                smallestentrophy = entropynow\n",
    "                bestattribute = {attribute:attributes[attribute]}\n",
    "            \n",
    "        elif attributes[attribute] == \"numerical\":\n",
    "            datasetsort = datasetbycolumn.T[datasetbycolumn.T[:,idx].argsort(kind='quicksort')].T\n",
    "            currentthreshold = (datasetsort[idx][1]+datasetsort[idx][0])/2\n",
    "            k = 1\n",
    "            while k < len(datasetsort.T):\n",
    "                currentthreshold = (datasetsort[idx][k]+datasetsort[idx][k-1])/2\n",
    "                listofcategory = [datasetsort[classindex][:k],datasetsort[classindex][k:]]\n",
    "                entropynow = 0\n",
    "\n",
    "                for ctgry in listofcategory:\n",
    "                    a = len(ctgry)/len(datasetbycolumn[idx]) # This is probability\n",
    "                    entropynow += a * entropy(ctgry)\n",
    "\n",
    "                if entropynow < smallestentrophy:\n",
    "                    smallestentrophy = entropynow\n",
    "                    thresholdvalue = currentthreshold\n",
    "                    bestattribute = {attribute:attributes[attribute]}    \n",
    "                k += 1\n",
    "        i += 1\n",
    "\n",
    "    gain = originalentrophy-smallestentrophy\n",
    "    # set first attribution dictionary {key:type} to the best attributes.\n",
    "    return bestattribute, thresholdvalue, gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1621,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropbyindex(data, category, listindex):\n",
    "    newdata = np.delete(data.T, listindex,0).T\n",
    "    keytoremove = [list(category.keys())[i] for i in listindex]\n",
    "    newcategory = category.copy()\n",
    "    [newcategory.pop(key) for key in keytoremove]\n",
    "    return newdata, newcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1622,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def decisiontree(dataset: np.array, dictattributes: dict, algortype: str ='id3'):\n",
    "    datasetcopy = np.copy(dataset).T # dataset copy is by colomn. \n",
    "    dictattricopy = dictattributes.copy()\n",
    "    classindex = list(dictattricopy.values()).index(\"class\")\n",
    "\n",
    "    def processbest(algor):\n",
    "        if algor == \"cart\" or algor == \"gini\":\n",
    "            return cartbestseperate(datasetcopy.T, dictattricopy)\n",
    "        else: # algor == \"id3\" or algor == \"infogain\"\n",
    "            return id3bestseperate(datasetcopy.T, dictattricopy)\n",
    "\n",
    "    node = Treenode(label=-1,type=\"decision\")\n",
    "\n",
    "    node.majority = majority(datasetcopy[classindex])\n",
    "\n",
    "    if same(datasetcopy[classindex]):\n",
    "        node.type = \"leaf\"\n",
    "        node.label = datasetcopy[classindex][0]\n",
    "        return node\n",
    "    \n",
    "    if len(dictattricopy) == 0:\n",
    "        node.type = \"leaf\"\n",
    "        node.label = majority(datasetcopy[classindex])\n",
    "        return node\n",
    "\n",
    "    bestattributedict,thresholdval = processbest(algortype)[:2]\n",
    "    bestattributename = list(bestattributedict.keys())[0]\n",
    "    bestattributetype = bestattributedict[bestattributename]\n",
    "    node.testattributedict = bestattributedict\n",
    "    node.datatype = bestattributetype\n",
    "    node.testattribute = bestattributename\n",
    "    node.threshold = thresholdval\n",
    "    bindex = list(dictattricopy.keys()).index(list(bestattributedict.keys())[0])\n",
    "\n",
    "    subdatalists = []\n",
    "    if bestattributetype == \"numerical\":\n",
    "        sortedcopy = datasetcopy.T[datasetcopy.T[:,bindex].argsort(kind='quicksort')].T\n",
    "        splitindex = 0\n",
    "        for numericalvalue in sortedcopy[bindex]:\n",
    "            if numericalvalue > thresholdval:\n",
    "                break\n",
    "            else:\n",
    "                splitindex += 1\n",
    "        subdatalistraw = [sortedcopy.T[:splitindex].T,sortedcopy.T[splitindex:].T]\n",
    "        for subdata in subdatalistraw:\n",
    "            subdatav = np.delete(subdata,bindex,0)\n",
    "            subdatalists.append(subdatav.T)\n",
    "    else:\n",
    "        bigv = list(Counter(datasetcopy[bindex]).keys()) # this is the all the categories of the test attribute left.\n",
    "    \n",
    "        for smallv in bigv:\n",
    "            index = [idx for idx, element in enumerate(datasetcopy[bindex]) if element == smallv]\n",
    "            subdatav = np.array(datasetcopy.T[index]).T\n",
    "            subdatav = np.delete(subdatav,bindex,0)  # I delete the column I already used using bindex as reference. \n",
    "            # Then, later, pop the same index from list attribute.\n",
    "            subdatalists.append(subdatav.T) # list of nparrays of target/label/categories.\n",
    "\n",
    "    dictattricopy.pop(bestattributename)\n",
    "    \n",
    "    edge = {}\n",
    "    sdindex = 0\n",
    "    for subvdata in subdatalists:\n",
    "        if subvdata.size == 0:\n",
    "            node.type = \"leaf\"\n",
    "            node.label = node.majority\n",
    "            node.threshold = thresholdval\n",
    "            return node\n",
    "\n",
    "        subtree = decisiontree(subvdata, dictattricopy, algortype)\n",
    "        if bestattributetype == 'numerical':\n",
    "            attributevalue = \"<=\" if sdindex == 0 else \">\"\n",
    "        else:\n",
    "            attributevalue = bigv[sdindex]\n",
    "\n",
    "        edge[attributevalue] = subtree\n",
    "        sdindex += 1\n",
    "\n",
    "    node.edge = edge\n",
    "\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1623,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisiontreeforest(dataset: np.array, dictattributes: dict, algortype: str ='id3', maxdepth: int = 10, minimalsize: int = 10, minimalgain: float = 0.01):\n",
    "    datasetcopy = np.copy(dataset).T # dataset copy is by colomn. \n",
    "    dictattricopy = dictattributes.copy()\n",
    "    classindex = list(dictattributes.values()).index(\"class\")\n",
    "    k = len(dictattributes)-1\n",
    "    randomlist = random.sample(range(0, k), round(math.sqrt(k))) if classindex !=0 else random.sample(range(1, k+1), round(math.sqrt(k)))\n",
    "    randomlist.append(classindex)\n",
    "    randomkey = [list(dictattricopy.keys())[i] for i in randomlist]\n",
    "    trimmeddict = {key:dictattricopy[key] for key in randomkey}\n",
    "    trimmeddata = np.array(datasetcopy[randomlist])\n",
    "\n",
    "    def processbest(algor):\n",
    "        if algor == \"cart\" or algor == \"gini\":\n",
    "            return cartbestseperate(trimmeddata.T, trimmeddict)\n",
    "        else: # algor == \"id3\" or algor == \"infogain\"\n",
    "            return id3bestseperate(trimmeddata.T, trimmeddict)\n",
    "\n",
    "    node = Treenode(label=-1,type=\"decision\")\n",
    "    currentdepth = node.depth\n",
    "\n",
    "    node.majority = majority(datasetcopy[classindex])\n",
    "\n",
    "    if same(datasetcopy[classindex]):\n",
    "        node.type = \"leaf\"\n",
    "        node.label = datasetcopy[classindex][0]\n",
    "        return node\n",
    "    \n",
    "    if len(dictattricopy) == 0:\n",
    "        node.type = \"leaf\"\n",
    "        node.label = majority(datasetcopy[classindex])\n",
    "        return node\n",
    "\n",
    "    # A stopping criteria  'minimal_size_for_split_criterion'\n",
    "\n",
    "    if len(dataset) <= minimalsize:\n",
    "        node.type = \"leaf\"\n",
    "        node.label = majority(datasetcopy[classindex])\n",
    "        return node\n",
    "\n",
    "    bestattributedict,thresholdval,gain = processbest(algortype)\n",
    "    bestattributename = list(bestattributedict.keys())[0]\n",
    "    bestattributetype = bestattributedict[bestattributename]\n",
    "    node.testattributedict = bestattributedict\n",
    "    node.datatype = bestattributetype\n",
    "    node.testattribute = bestattributename\n",
    "    node.threshold = thresholdval\n",
    "    bindex = list(dictattricopy.keys()).index(list(bestattributedict.keys())[0])\n",
    "\n",
    "    # A Possible Stopping criteria 'minimal_gain'\n",
    "\n",
    "    if gain < minimalgain:\n",
    "        node.type = \"leaf\"\n",
    "        node.label = majority(datasetcopy[classindex])\n",
    "        return node\n",
    "\n",
    "    subdatalists = []\n",
    "    if bestattributetype == \"numerical\":\n",
    "        sortedcopy = datasetcopy.T[datasetcopy.T[:,bindex].argsort(kind='quicksort')].T\n",
    "        splitindex = 0\n",
    "        for numericalvalue in sortedcopy[bindex]:\n",
    "            if numericalvalue > thresholdval:\n",
    "                break\n",
    "            else:\n",
    "                splitindex += 1\n",
    "        subdatalistraw = [sortedcopy.T[:splitindex].T,sortedcopy.T[splitindex:].T]\n",
    "        for subdata in subdatalistraw:\n",
    "            subdata = np.delete(subdata,bindex,0)\n",
    "            subdatalists.append(subdata.T)\n",
    "    else:\n",
    "        bigv = list(Counter(datasetcopy[bindex]).keys()) # this is the all the categories of the test attribute left.\n",
    "    \n",
    "        for smallv in bigv:\n",
    "            index = [idx for idx, element in enumerate(datasetcopy[bindex]) if element == smallv]\n",
    "            subdatav = np.array(datasetcopy.T[index]).T\n",
    "            subdatav = np.delete(subdatav,bindex,0)  # I delete the column I already used using bindex as reference. \n",
    "            # Then, later, pop the same index from list attribute.\n",
    "            subdatalists.append(subdatav.T) # list of nparrays of target/label/categories.\n",
    "\n",
    "    dictattricopy.pop(bestattributename)\n",
    "    \n",
    "    edge = {}\n",
    "    sdindex = 0\n",
    "    for subvdata in subdatalists:\n",
    "\n",
    "        if subvdata.size == 0:\n",
    "            node.type = \"leaf\"\n",
    "            node.label = node.majority\n",
    "            node.threshold = thresholdval\n",
    "            return node\n",
    "\n",
    "        # Another Stoping criteria I could ADD: maximal depth\n",
    "        \n",
    "        if node.caldepth()+1 > maxdepth:  \n",
    "            node.type = \"leaf\"\n",
    "            node.label = node.majority\n",
    "            node.threshold = thresholdval\n",
    "            return node \n",
    "\n",
    "        subtree = decisiontreeforest(subvdata, dictattricopy, algortype, maxdepth, minimalsize, minimalgain)\n",
    "        subtree.depth = currentdepth + 1\n",
    "        subtree.parent = node\n",
    "            \n",
    "        if bestattributetype == 'numerical':\n",
    "            attributevalue = \"<=\" if sdindex == 0 else \">\"\n",
    "        else:\n",
    "            attributevalue = bigv[sdindex]\n",
    "\n",
    "        edge[attributevalue] = subtree\n",
    "        sdindex += 1\n",
    "\n",
    "    node.edge = edge\n",
    "\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1624,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(data, rand=691):\n",
    "    train, test = sklearn.model_selection.train_test_split(data, train_size=0.8, test_size=0.2, random_state=rand, shuffle=True)\n",
    "    return train,test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1625,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(tree: Treenode, instance, dictattricopy): # note that the instance is by row. (I formerly used by column)\n",
    "    predict = tree.majority\n",
    "    classindex = list(dictattricopy.values()).index(\"class\")\n",
    "    correct = instance[classindex]\n",
    "    if tree.type == 'leaf':\n",
    "        predict = tree.label\n",
    "        return predict, correct, predict==correct\n",
    "\n",
    "    testindex = list(dictattricopy.keys()).index(tree.testattribute)\n",
    "    \n",
    "    if tree.datatype == \"numerical\":\n",
    "        if instance[testindex] <= tree.threshold:\n",
    "            nexttree = tree.edge['<=']\n",
    "        else:\n",
    "            nexttree = tree.edge['>']\n",
    "    else:\n",
    "        if instance[testindex] not in tree.edge:\n",
    "            return predict, correct, predict==correct\n",
    "            \n",
    "        nexttree = tree.edge[instance[testindex]]\n",
    "\n",
    "    return prediction(nexttree, instance, dictattricopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(data, ratio=0.1): \n",
    "    data2 = np.copy(data)\n",
    "    k = len(data)\n",
    "    randomlist = random.sample(range(0, k), round(k*ratio))\n",
    "    data2 = np.delete(data2, randomlist, 0)\n",
    "    p = len(data2)\n",
    "    randomfill = random.sample(range(0, p), k-p)\n",
    "    data2 = np.concatenate((data2,data2[randomfill]),0)\n",
    "    # print(len(data2))\n",
    "    return data2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratifiedkfold(data, categorydict, k = 10):\n",
    "    classindex = list(categorydict.values()).index(\"class\")\n",
    "    datacopy = np.copy(data).T\n",
    "    classes = list(Counter(datacopy[classindex]).keys())\n",
    "    nclass = len(classes) # number of classes\n",
    "    listofclasses = []\n",
    "\n",
    "    for oneclass in classes:\n",
    "        index = [idx for idx, element in enumerate(datacopy[classindex]) if element == oneclass]\n",
    "        oneclassdata = np.array(datacopy.T[index])\n",
    "        np.random.shuffle(oneclassdata)\n",
    "        listofclasses.append(oneclassdata)\n",
    "\n",
    "    splitted = [np.array_split(i, k) for i in listofclasses]\n",
    "    nclass = len(classes)\n",
    "    combined = []\n",
    "\n",
    "    for j in range(k):\n",
    "        ithterm = []\n",
    "        for i in range(nclass):\n",
    "            if ithterm == []:\n",
    "                ithterm = splitted[i][j]\n",
    "            else:\n",
    "                ithterm = np.append(ithterm,splitted[i][j],0)\n",
    "        combined.append(ithterm)\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1628,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeindex= []\n",
    "d1,c1=dropbyindex(cmcdata,cmccategory,removeindex)\n",
    "\n",
    "train1, test1 = split_test_train(d1)\n",
    "traintree = decisiontreeforest(train1,c1,'gini')\n",
    "#print(traintree.edge['>'].edge['>'].edge['<='].edge['<='].caldepth())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4610169491525424\n"
     ]
    }
   ],
   "source": [
    "correctcount = 0\n",
    "# print(c1)\n",
    "for instance in test1:\n",
    "    if prediction(traintree,instance,c1)[2]:\n",
    "        correctcount+=1\n",
    "\n",
    "accrate = correctcount/len(test1)\n",
    "print(accrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1630,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plantforest(data, categorydict, ntree=10, maxdepth=10, minimalsize=10, minimalgain=0.05, algortype='id3'):\n",
    "    forest = []\n",
    "    for i in range(ntree):\n",
    "        datause = bootstrap(data, 0.05)\n",
    "        tree = decisiontreeforest(datause,categorydict,algortype,maxdepth,minimalsize,minimalgain)\n",
    "        forest.append(tree)\n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1631,
   "metadata": {},
   "outputs": [],
   "source": [
    "myforest = plantforest(housedata,housecategory,ntree=50,maxdepth=10,minimalsize=10,minimalgain=0.05,algortype='id3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<decisiontree.Treenode at 0x1486d9120>,\n",
       " <decisiontree.Treenode at 0x148279570>,\n",
       " <decisiontree.Treenode at 0x1481592d0>,\n",
       " <decisiontree.Treenode at 0x138a02440>,\n",
       " <decisiontree.Treenode at 0x1486da2f0>,\n",
       " <decisiontree.Treenode at 0x1486da650>,\n",
       " <decisiontree.Treenode at 0x1486d9150>,\n",
       " <decisiontree.Treenode at 0x1486d9180>,\n",
       " <decisiontree.Treenode at 0x1486d90f0>,\n",
       " <decisiontree.Treenode at 0x1484748b0>,\n",
       " <decisiontree.Treenode at 0x148474ca0>,\n",
       " <decisiontree.Treenode at 0x148475150>,\n",
       " <decisiontree.Treenode at 0x1484745b0>,\n",
       " <decisiontree.Treenode at 0x148474340>,\n",
       " <decisiontree.Treenode at 0x148475b40>,\n",
       " <decisiontree.Treenode at 0x148475ba0>,\n",
       " <decisiontree.Treenode at 0x148476620>,\n",
       " <decisiontree.Treenode at 0x148476a70>,\n",
       " <decisiontree.Treenode at 0x148477220>,\n",
       " <decisiontree.Treenode at 0x148477460>,\n",
       " <decisiontree.Treenode at 0x1484774f0>,\n",
       " <decisiontree.Treenode at 0x148477eb0>,\n",
       " <decisiontree.Treenode at 0x1484779a0>,\n",
       " <decisiontree.Treenode at 0x148477dc0>,\n",
       " <decisiontree.Treenode at 0x148400b80>,\n",
       " <decisiontree.Treenode at 0x148400a00>,\n",
       " <decisiontree.Treenode at 0x148401cc0>,\n",
       " <decisiontree.Treenode at 0x148401ea0>,\n",
       " <decisiontree.Treenode at 0x1484025f0>,\n",
       " <decisiontree.Treenode at 0x148402710>,\n",
       " <decisiontree.Treenode at 0x148402e30>,\n",
       " <decisiontree.Treenode at 0x148403070>,\n",
       " <decisiontree.Treenode at 0x148403250>,\n",
       " <decisiontree.Treenode at 0x148403af0>,\n",
       " <decisiontree.Treenode at 0x1484036d0>,\n",
       " <decisiontree.Treenode at 0x148403c70>,\n",
       " <decisiontree.Treenode at 0x148329390>,\n",
       " <decisiontree.Treenode at 0x148328a00>,\n",
       " <decisiontree.Treenode at 0x1483294b0>,\n",
       " <decisiontree.Treenode at 0x148329630>,\n",
       " <decisiontree.Treenode at 0x148329a50>,\n",
       " <decisiontree.Treenode at 0x14832a170>,\n",
       " <decisiontree.Treenode at 0x14832a0e0>,\n",
       " <decisiontree.Treenode at 0x14832abf0>,\n",
       " <decisiontree.Treenode at 0x14832abc0>,\n",
       " <decisiontree.Treenode at 0x14832bc10>,\n",
       " <decisiontree.Treenode at 0x14832b5b0>,\n",
       " <decisiontree.Treenode at 0x14832bc70>,\n",
       " <decisiontree.Treenode at 0x1483d09a0>,\n",
       " <decisiontree.Treenode at 0x1483d0130>]"
      ]
     },
     "execution_count": 1632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myforest"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
